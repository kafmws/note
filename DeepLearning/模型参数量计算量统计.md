
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [模型参数量/计算量统计](#模型参数量计算量统计)
  - [已有方法](#已有方法)
    - [直接统计](#直接统计)
    - [torchsummary](#torchsummary)
    - [torchstat](#torchstat)
    - [torchinfo](#torchinfo)
  - [竞品分析](#竞品分析)

<!-- /code_chunk_output -->



# 模型参数量/计算量统计

统计如下模型的参数量/计算量

```Python
import torch
import torch.nn as nn
import torchsummary


class Net(nn.Module):
    def __init__(self):
        super(Net,self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)
        self.conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1,padding=1, bias=False)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        return x
```

## 已有方法

### 直接统计

**Usage:**
```Python
def count_parameters(model):
    """ 统计模型每一层中需要计算梯度的参数的数量 """
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


print(f'plain trainable parameters count: {count_parameters(model)}')
```

**Output:**
```
trainable parameters count: 18
```


### torchsummary

**Usage:**
```Python
torchsummary.summary(model.cuda() if torch.cuda.is_available() else model.cpu(), 
    input_size=(1, 512, 512), batch_size=-1, device='cuda' if torch.cuda.is_available() else 'cpu')
```

**Output:**
```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1          [-1, 1, 512, 512]               9
            Conv2d-2          [-1, 1, 512, 512]               9
================================================================
Total params: 18
Trainable params: 18
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 1.00
Forward/backward pass size (MB): 4.00
Params size (MB): 0.00
Estimated Total Size (MB): 5.00
----------------------------------------------------------------
```

### torchstat

**Usage:**
```Python
torchstat.stat(model.cpu(), (1, 512, 512))
```

**Output:**
```
    module name  input shape output shape  params memory(MB)         MAdd        Flops  MemRead(B)  MemWrite(B) duration[%]  MemR+W(B)
0           conv1    1 512 512    1 512 512     9.0       1.00  4,456,448.0  2,359,296.0   1048612.0    1048576.0      60.84%  2097188.0
1           conv2    1 512 512    1 512 512     9.0       1.00  4,456,448.0  2,359,296.0   1048612.0    1048576.0      39.15%  2097188.0
total                                          18.0       2.00  8,912,896.0  4,718,592.0   1048612.0    1048576.0     100.00%  4194376.0
========================================================================================================================================
Total params: 18
----------------------------------------------------------------------------------------------------------------------------------------
Total memory: 2.00MB
Total MAdd: 8.91MMAdd
Total Flops: 4.72MFlops
Total MemR+W: 4.0MB
```

### torchinfo

**Usage:**
```Python
torchinfo.summary(model.cuda(), (1, 512, 512))
```

**Output:**
```
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Net                                      [1, 512, 512]             --
├─Conv2d: 1-1                            [1, 512, 512]             9
├─Conv2d: 1-2                            [1, 512, 512]             9
==========================================================================================
Total params: 18
Trainable params: 18
Non-trainable params: 0
Total mult-adds (M): 0.01
==========================================================================================
Input size (MB): 1.05
Forward/backward pass size (MB): 4.19
Params size (MB): 0.00
Estimated Total Size (MB): 5.24
==========================================================================================
```

## 竞品分析

- 共享参数情况


```Python
class Net(nn.Module):
    def __init__(self):
        super(Net,self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)
        # self.conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv1(x)   # virtual conv2 use the same params with conv1
        return x
```


**Result:**
```
plain:
    plain trainable parameters count: 9
    
    
torchsummary:
    ----------------------------------------------------------------
            Layer (type)               Output Shape         Param #
    ================================================================
                Conv2d-1          [-1, 1, 512, 512]               9
                Conv2d-2          [-1, 1, 512, 512]               9
    ================================================================
    Total params: 18
    Trainable params: 18
    Non-trainable params: 0
    ----------------------------------------------------------------
    Input size (MB): 1.00
    Forward/backward pass size (MB): 4.00
    Params size (MB): 0.00
    Estimated Total Size (MB): 5.00
    ----------------------------------------------------------------
    
    
torchstat:
            module name  input shape output shape  params memory(MB)         MAdd        Flops  MemRead(B)  MemWrite(B) duration[%]  MemR+W(B)
    0           conv1    1 512 512    1 512 512     9.0       1.00  4,456,448.0  2,359,296.0   1048612.0    1048576.0      99.99%  2097188.0
    total                                           9.0       1.00  4,456,448.0  2,359,296.0   1048612.0    1048576.0      99.99%  2097188.0
    ========================================================================================================================================
    Total params: 9
    ----------------------------------------------------------------------------------------------------------------------------------------
    Total memory: 1.00MB
    Total MAdd: 4.46MMAdd
    Total Flops: 2.36MFlops
    Total MemR+W: 2.0MB
    

torchinfo:
    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    Net                                      [1, 512, 512]             --
    ├─Conv2d: 1-1                            [1, 512, 512]             9
    ├─Conv2d: 1-2                            [1, 512, 512]             (recursive)
    ==========================================================================================
    Total params: 9
    Trainable params: 9
    Non-trainable params: 0
    Total mult-adds (M): 0.01
    ==========================================================================================
    Input size (MB): 1.05
    Forward/backward pass size (MB): 2.10
    Params size (MB): 0.00
    Estimated Total Size (MB): 3.15
    ==========================================================================================
```

- 存在未使用层

```Python
class Net(nn.Module):
    def __init__(self):
        super(Net,self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)
        self.conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)  # unused layer

    def forward(self, x):
        x = self.conv1(x)
        # x = self.conv2(x)
        return x
```


**Result:**
```
plain:    
    plain trainable parameters count: 18


torchsummary:
    ----------------------------------------------------------------
            Layer (type)               Output Shape         Param #
    ================================================================
                Conv2d-1          [-1, 1, 512, 512]               9
    ================================================================
    Total params: 9
    Trainable params: 9
    Non-trainable params: 0
    ----------------------------------------------------------------
    Input size (MB): 1.00
    Forward/backward pass size (MB): 2.00
    Params size (MB): 0.00
    Estimated Total Size (MB): 3.00
    ----------------------------------------------------------------
    
    
torchstat:
          module name  input shape output shape  params memory(MB)         MAdd        Flops  MemRead(B)  MemWrite(B) duration[%]  MemR+W(B)
    0           conv1    1 512 512    1 512 512     9.0       1.00  4,456,448.0  2,359,296.0   1048612.0    1048576.0     100.00%  2097188.0
    1           conv2    0   0   0    0   0   0     0.0       0.00          0.0          0.0         0.0          0.0       0.00%        0.0
    total                                           9.0       1.00  4,456,448.0  2,359,296.0         0.0          0.0     100.00%  2097188.0
    ========================================================================================================================================
    Total params: 9
    ----------------------------------------------------------------------------------------------------------------------------------------
    Total memory: 1.00MB
    Total MAdd: 4.46MMAdd
    Total Flops: 2.36MFlops
    Total MemR+W: 2.0MB
    

torchinfo:
    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    Net                                      [1, 512, 512]             9
    ├─Conv2d: 1-1                            [1, 512, 512]             9
    ==========================================================================================
    Total params: 18
    Trainable params: 18
    Non-trainable params: 0
    Total mult-adds (M): 0.00
    ==========================================================================================
    Input size (MB): 1.05
    Forward/backward pass size (MB): 2.10
    Params size (MB): 0.00
    Estimated Total Size (MB): 3.15
    ==========================================================================================
```

**结论：**
`torchsummary`仅根据网络传播过程统计参数，但会重复统计共享参数
`torchstat`综合考量了网络定义和具体传播过程，不会重复统计共享参数，但共享参数及计算量只会计算一次
`torchinfo`综合考量了网络定义和具体传播过程，不会重复统计共享参数，能够输出共享参数的层，但会统计未使用层的参数

严谨的代码不应该出现不使用的层，因此使用`torchinfo`统计参数量、使用`torchstat`统计计算量较为合适，并且需要特别考虑存在共享参数的情况。