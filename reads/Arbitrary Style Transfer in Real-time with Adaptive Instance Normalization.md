# 自适应实例标准化实现实时任意风格迁移

现有的风格迁移算法迭代优化过程缓慢，限制实际应用。基于前向反馈神经网络的快速逼近算法取得了更好的速度，但将网络同一组固定的风格绑定，不能实现任意风格的迁移。本文提出了迄今为止首个实现实时任意风格迁移的方法。核心方法是自适应实例标准化层（Adapative Instance Normalization layer）。AdaIN 层将原图特征的均值和方差对齐到风格特征的均值和方差。本文实现了单个前向反馈神经网络进行任意而不是预定义的一组风格的迁移，包括控制目标风格的比例，风格插值、色彩和空间控制，并与现存最快方法的速度相近。


## 1. Introdution
Gatys等人证明了DNN同时提取了图片的内容和风格两种信息，在某种程度上二者是可分的，因此保留内容而改变风格具有可能性。他们提出的方法能够将任意图片的风格迁移到其它图片，但因为需要一个迭代式的优化过程导致速度太慢。

有一些工作使用前向反馈神经网络的单次前向传播实现风格化。大多数前向反馈方法的主要局限在于每个网络限制于一个风格，近期有些工作拓宽到了一组风格，或者迁移任意风格但速度远远慢于单个风格。

受效果极好的实例规范化（层）的启发，我们解决了这个灵活性和速度的困境。实例规范化通过规范承载图像风格信息的特征统计数据实现风格规范化，根据这一理解，我们将IN拓展为AdaIN。AdaIN根据风格输入的均值和方差调整内容输入的方差和均值，学习出的解码器再将AdaIN的输出反向解码到图像空间生成风格化图像。比原始的DNN方法快了3个数量级。

## 2. Related Work

- 风格迁移：

风格迁移起源于非照片真实感渲染，与纹理合成及迁移密切相关。一些早期的方法如线性滤波器响应的直方图匹配和非参数化采样，依赖于低层次统计数据难以获取图像的语义结构。Gatys等人通过匹配DNN卷积层中的特征数据首次给出了令人印象深刻的风格迁移结果。Li and Wand在深层特征空间中引入了马尔科夫随机场来增强局部模式。Gatys 等人提出了色彩保留、空间位置和风格迁移的尺度的控制方法。Ruder等人通过施加时间约束提高了视频风格迁移的质量。
Gatys等人的框架基于一个迭代优化过程，通过损失网络最小化内容损失和风格损失。需要分钟级的时间才能收敛。

Ulyanov等提出了改善和提高生成图像的质量及多样性的方法。
Chen and Schmidt等提出了能迁移到任意风格的前馈网络，其中的风格交换层用匹配最接近的风格特征以块到块的方式替换内容特征。尽管如此，风格交换层造成了新的计算瓶颈：对于521x512的输入图像，风格交换产生了超过95%的计算开销。我们的方法在允许任意风格迁移的同时比此方法快1~2个数量级。

风格迁移的另一个核心问题是使用哪种风格损失函数。最初Gatys等人通过匹配Gram矩阵捕获的特征激活图之间的二阶统计量来匹配风格。其他有效的损失函数有马尔科夫随机场损失，对抗损失，直方图损失，CORAL loss，MMD loss 和通道均值和方差之间的距离。上述所有损失函数目的都是匹配风格图像和合成图像之间的某些特征统计信息。

- 深度生成图像建模

有一些用于图像生成的通用框架，包括VAE，自回归模型和GAN。其中GAN实现了最好的效果。许多GAN的改进方案如条件生成，多阶段处理和更好的训练目标被提出。GAN也被用于风格迁移和跨域图像生成。

## Background

### 3.1 Batch Normalization

Ioffe and Szegedy提出的批量规范化影响深远，通过规范化特征统计量使FFN的训练变得容易。BN最初设计用于加速判别网络的训练，但也被发现在图像生成模型中有效。

给定一个batch $ x \in \mathbb{R}^{N\times C \times H \times W} $ 作为输入，BN 将每个通道的均值和标准差规范化。
\[
    BN(x) = \gamma(\frac{x-\mu(x)}{\sigma(x)} + \beta)
\]
其中，$ \gamma, \beta \in \mathbb{R}^C $是从数据中得到的仿射参数。$ \mu(x), \sigma(x) \in \mathbb{R}^C $是均值和标准差，每个特征通道的均值和标准差由 batch 的大小和空间维度计算得到：
\[
    \mu_c(x) = \frac{1}{NHW}\sum_{n=1}^N\sum_{h=1}^H\sum_{w=1}^Wx_{nchw}
    \\
    \sigma_c(x) = \sqrt{\frac{1}{NHW}\sum_{n=1}^N\sum_{h=1}^H\sum_{w=1}^W(x_{nchw}-\mu_c(x))^2 + \epsilon}  
\]
BN训练时使用mini-batch的统计数据，推理时使用全局统计数据，存在不一致。近来有 Batch Renormalization(BRN) 被提出在训练阶段逐渐过渡到全局统计数据。

> style transfer 中很看重 $\mu, \sigma$，channel pruning 中比较看重 $\gamma, \beta$
（BRN在batch size比较小、batch的统计数据非独立同分布情况下效果明显）
BRN 认为在用每个batch的均值和方差来代替整体训练集的均值和方差时，可以再通过一个线性变换来逼近数据的真实分布
\[
    \frac{x_i - \mu}{\sigma} = \frac{x_i - \mu_{batch}}{\sigma_{batch}} \gamma + d
\]
而当
\[
    \gamma = \frac{\sigma_{batch}}{\sigma}, d = \frac{\mu_{batch} - \mu}{\sigma}
\]
时，就达到了理想的效果。

B